<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Compositional Visual Generation and Inference with Energy Based Models</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="fig/comp_cartoon.jpg"/>
<meta property="og:title" content="Compositional Visual Generation and Inference with Energy Based Models" />

<script src="lib.js" type="text/javascript"></script>
<script src="popup.js" type="text/javascript"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
    PADDING-RIGHT: 0px;
    PADDING-LEFT: 0px;
    FLOAT: right;
    PADDING-BOTTOM: 0px;
    PADDING-TOP: 0px
}
#primarycontent {
    MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
    TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="b5m.js" id="b5mmain" type="text/javascript"></script></head>

<body>

<div id="primarycontent">
<center><h1>Compositional Visual Generation and Inference with Energy Based Models</h1></center>
<center><h2><a href="https://yilundu.github.io/">Yilun Du<sup>1</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="http://www.mit.edu/~lishuang/">Shuang Li<sup>1</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://openai.com/blog/authors/igor/">Igor Mordatch<sup>2</sup></a></h2></center>
<center><h2>
  <sup>1</sup> MIT CSAIL&nbsp;&nbsp;&nbsp;
  <sup>2</sup> Google Brain
</h2></center>
<center><h2><strong><a href="">Paper</a> | <a href="">Tensorflow code</a></strong> </h2></center>
<br>


<center><a>
<img src="fig/comp_cartoon.jpg" width="1000"> 
</a></center>
<br>



<p>
<h2>Abstract</h2>

<div style="font-size:14px"><p align="justify">
  A vital aspect of human intelligence is the ability to compose increasingly complex concepts out of simpler ideas, enabling both rapid learning and adaptation of knowledge. In this paper we show that energy-based models can exhibit this ability by directly combining probability distributions. Samples from the combined distribution correspond to compositions of concepts. For example, given a distribution for smiling faces, and another for male faces, we can combine them to generate smiling male faces. This allows us to generate natural images that simultaneously satisfy conjunctions, disjunctions, and negations of concepts. We evaluate compositional generation abilities of our model on the CelebA dataset of natural faces and synthetic 3D scene images. We also demonstrate other unique advantages of our model, such as the ability to continually learn and incorporate new concepts, or infer compositions of concept properties underlying an image.
</div>
</p>



<!-- ---------------------------------------------------------------------------------- -->
<a href=""><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="fig/paper_thumbnail.jpg" width=170></a>
<br>


<h2>Paper</h2>
<p><a href="fig/paper.pdf">arxiv</a>,  2019. </p>



<h2>Citation</h2>
<p>"Compositional Visual Generation and Inference with Energy Based Models", arxiv.
<a href="fig/citation.txt">Bibtex</a>

</p>


<h2>Code: <a href=''>Tensorflow</a> </h2>

<br><br>

<!-- ---------------------------------------------------------------------------------- -->
<h2>Method</h2>

Energy based models (EBMs) represent a distribution over data by defining an energy \(E_\theta(x) \) so that the likelihood of the data is proportional to   \( \propto e^{-E_\theta(x)}\).


We can generate data from an EBM by implicit sampling through Langevin dynamics, where samples are sequentially refined, following the procedure
\[ \tilde{\mathbb{x}}^k = \tilde{\mathbb{x}}^{k-1} - \frac{\lambda}{2} \nabla_\mathbb{x} E_\theta (\tilde{\mathbb{x}}^{k-1}) + \omega^k, \; \omega^k \sim \mathcal{N}(0,\lambda), \]


By defining generation through such a manner, we can compose generation across different EBMs learned on attributes of position, size, color, gender, hair style, and age, through the symbolic operators of conjunction, disjunction, and negation. 

In particular, we consider a set of independently trained EBMs, \(E(\tilde{x}|c_1), E(\tilde{x}|c_2), \ldots,  E(\tilde{x}|c_n)\), which are learned conditional distributions on underlying latent codes $c_i$. Latent codes we consider include position, size, color, gender, hair style, and age, which we also refer to as concepts.

<h3>Conjunction</h3>

In concept conjunction, given separate independent concepts (such as a particular gender, hair style, or facial expression), we wish to construct an generation with the specified gender, hair style, and facial expression -- the combination of each concept. The likelihood of such an generation given a set of specific concepts is equal to the product of the likelihood of each individual concept

\[ p(x|c_1 \; \text{and} \; c_2, \ldots, \; \text{and} \; c_i) = \prod_i p(x|c_i) \propto e^{-\sum_i E(x|c_i)}. \]

Through our implicit sampling procedure, we can generate samples using 

\[ \tilde{\mathbb{x}}^k = \tilde{\mathbb{x}}^{k-1} - \frac{\lambda}{2} \nabla_\mathbb{x} \sum_i E_\theta (\tilde{\mathbb{x}}^{k-1}|c_i) + \omega^k.  \]


<h3>Disjunction</h3>

In concept disjunction, given separate concepts such as the colors red and blue, we wish to construct an output that is either red or blue. We wish to construct a new distribution that has probability mass when any chosen concept is true. A natural choice of such a distribution is the sum of the likelihood of each concept:

\[ p(x|c_1 \; \text{or} \; c_2, \ldots \; \text{or}  \; c_i) \propto \sum_i p(x|c_i) / Z(c_i). \]

where \( Z(c_i) \) denotes partition function for the chosen concept.

Through our implicit sampling procedure, by assuming partition functions are equal, we can then generate samples using  

\[ \tilde{\mathbb{x}}^k = \tilde{\mathbb{x}}^{k-1} - \frac{\lambda}{2} \nabla_\mathbb{x} \text{logsumexp}(-E(x|c_i))  + \omega^k \]

<h3>Negation</h3>

In concept negation, we wish to generate an output that does not contain the concept. Given a color red, we want an output that is of a different color, such as blue. Thus, we want to construct a distribution that places high likelihood to data that is outside a given concept. One choice is a distribution inversely proportional to the concept. Importantly, negation must be defined with respect to another concept to be useful. The opposite of alive may be dead, but not inanimate. Negation without a data distribution is not integrable and leads to a generation of chaotic textures which, while satisfying absence of a concept, is not desirable. Thus in our experiments with negation we combine it with another concept to ground the negation and obtain an integrable distribution:

\[ p(x| \text{not}(c_1), c_2) \propto \frac{p(x|c_2)}{p(x|c_1)^\alpha} \propto e^{ \alpha  E(x|c_1) - E(x|c_2) } \]

Through our implicit sampling procedure, by assuming partition functions are equal, we can then generate samples using

\[ \tilde{\mathbb{x}}^k = \tilde{\mathbb{x}}^{k-1} - \frac{\lambda}{2} \nabla_\mathbb{x} (\alpha E(x|c_1) - E(x|c_2)) + \omega^k \]


By definely particular combinations of EBMs trained on male, smiling, and black haired faces, we are able to obtain generations shown below:

<img src="fig/venn.jpg" width="800"> </a></center>



<!-- ---------------------------------------------------------------------------------- -->
<h2>Compositional Generations</h2>

We explore the ability of our models to compose across different attributes. We first condition on the attributes of shape, position, size and color attributes. Through the logical operator of conjunction, we can sequentially make more refined cube shapes through four independently trained EBMs.

<img src="fig/com_cube.jpg" width="300"> </a></center>

We can similarily condition different attributes of humans faces
<img src="fig/com_human.jpg" width="300"> </a></center>

Surprisingly, we find that by composing additional models together, we are actual able to make higher resolution images.

<h3>Higher Level Compositions</h3>

An advantage of defining composition through the manipulation of probability distributions, is that we can nest these operators to specify particular generation. We showcase this composing specific identites over multiple different attributes.

<img src="fig/symbolic.jpg" width="300"> </a></center>

<h3>Object Level Compositionality</h3>

In addition, we can also learn EBMs on the position attributes! Then, by applying conjunction to models, we further generate different compositions at the object level.

<img src="fig/multiobject.jpg" width="300"> </a></center>

By conditioning generation on sum of EBMs on two different positions, we can get two cube generations in different locations.


<h2>Continual Learning in Generation</h2>

By composing independent models sequentially, we can further use our learned models to generalize and extrapolate to a newly learned concepts. To test this we consider:

<ul id='continual_dataset'>
<li font-size: 15px>
A dataset consisting of position annotations of  purple cubes at different positions.
</li>

<li font-size: 15px>
A dataset consisting of shape annotations of different purple shapes at different positions
</li>

<li font-size: 15px>
A dataset consisting of color annotations of different color shapes  at different position
</li>
</ul>

We train a new EBM model for attributes of position, shape and color, and find that composing our three attribute EBMs together, we can precisely generate shapes of different position, shape and color objects! This is inspite of the factor that position and shape models have not seen such combinations of data.

<img src="fig/continuous.jpg" width="300"> </a></center>


<h2>Compositional Inference</h2>

<h3>Inference</h3>
In concept inference, we wish to infer the underlying concepts that best explains a given image. Given a learned EBM on \( E(x|c) \), we do inference to find the underlying concept \( c \) by

\[ c = \text{argmin}_c E(x|c) \]

If we know that a set of different \( x_i \) that all have the same underlying concept \( c \), we can then use conjunction to obtain , 

\[ c = \text{argmin}_c \sum_i E(x_i|c) \]

<h3>Compositional Inference Across Multiple Views</h3>

We test the above compositional reasoning on inferring the position of cube given different view of the image. By doing inference in such a manner, we find that we can obtain better predictions of positions.

<img src="fig/multiobject.jpg" width="300"> </a></center>

<h3>Compositional Inference In One Image</h3>

We can further test the ability of our model to compositionally infer the positions of multiple cubes when the model is trained on scenes with a single cube at different positions. A single EBM can compositionally infer the presence of multiple of cubes. 

<img src="fig/multiobject_compositionality.jpg" width="300"> </a></center>


<br><br>
<h2>Related Work</h2>

Here are some of our additional works on utilizing energy models:

<ul id='relatedwork'>
<li font-size: 15px>
 Yilun Du, Igor Mordatch <a href="https://papers.nips.cc/paper/8619-implicit-generation-and-modeling-with-energy-based-models"><strong>"Implicit Generation and Modeling with Energy Based Models"</strong></a>, in NeurIPS 2019 (Spotlight).
</li>
<li font-size: 15px>
 Yilun Du, Toru Lin, Igor Mordatch <a href="https://arxiv.org/abs/1909.06878"><strong>"Modeling Based Planning with Energy Based Models"</strong></a>, in CORL 2019.
</li>
<li font-size: 15px>
 Yilun Du, Joshua Meier, Jerry Ma, Rob Fergus, Alexander Rives <a href="https://openreview.net/pdf?id=S1e_9xrFvS"><strong>"Energy-Based Models For Atomic-Resolution
Protein Conformations"</strong></a>, in ICLR 2020 (Spotlight).
</li>
</ul>




<br>
<h2>Acknowledgement</h2>
<p align="justify"></p>

<div style="display:none">
<script type="text/javascript" src="http://gostats.com/js/counter.js"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script>
<noscript><a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"><img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" /></a></noscript>
</div>
</body></html
>

